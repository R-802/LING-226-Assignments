{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R-802/LING-226-Assignments/blob/main/Assignment_One.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDF9NZcFWpFx"
      },
      "source": [
        "#LING226 2023 T3 Assignment One\n",
        "- Shemaiah Rangitaawa `300601546`\n",
        "- Attempting Challenge\n",
        "\n",
        "**Note:** Please ensure you are in a GPU runtime environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXj8RZ3HXJnX"
      },
      "source": [
        "## **Text Preprocessing**\n",
        "Rather than removing terms by frequency, I have decided to remove text based on embedding similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Setting up the device for GPU usage\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Loading the tokenizer and model\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Checking if CUDA is available and getting the GPU device name\n",
        "cuda_available = torch.cuda.is_available()\n",
        "gpu_name = torch.cuda.get_device_name(0) if cuda_available else \"No CUDA Device Available\"\n",
        "\n",
        "cuda_available, gpu_name"
      ],
      "metadata": {
        "id": "1HGYNMQ006gG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05089a8-5f1d-48c9-d017-3f6f443fe926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 'Tesla T4')"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create BERT embeddings\n",
        "def create_embeddings(text, model=model, tokenizer=tokenizer, device='cuda'):\n",
        "    \"\"\"\n",
        "    Generate BERT embeddings for a given text.\n",
        "\n",
        "    :param text: The input text to generate embeddings for.\n",
        "    :param model: The BERT model (e.g., a pre-trained BERT model).\n",
        "    :param tokenizer: The BERT tokenizer.\n",
        "    :param device: The device (e.g., 'cuda' for GPU or 'cpu') to run the model on.\n",
        "\n",
        "    :return: A PyTorch Tensor containing the BERT embeddings for the input text.\n",
        "    \"\"\"\n",
        "    # Tokenize the text\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512, add_special_tokens=True)\n",
        "\n",
        "    # Move inputs to the specified device\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    # Put the model in \"evaluation\" mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get the model outputs\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the hidden states from the outputs\n",
        "    hidden_states = outputs.last_hidden_state\n",
        "\n",
        "    # Use the [CLS] token representation as the text embedding\n",
        "    text_embedding = hidden_states.mean(dim=1).squeeze(0)\n",
        "\n",
        "    return text_embedding"
      ],
      "metadata": {
        "id": "c7hjwFufI0dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess text by converting it to lowercase, removing punctuation, and filtering out stop words."
      ],
      "metadata": {
        "id": "ehB3devPFp5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# Function to preprocess text: lowercase, remove punctuation, and stop words\n",
        "def preprocess_text(text, stop_words):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = text.split()\n",
        "    return ' '.join([word for word in words if word not in stop_words])"
      ],
      "metadata": {
        "id": "gwIe9ECr46Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import torch.nn.functional as F\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Main function to refine text\n",
        "def refine_text(text, keyword_embeddings, stop_words, threshold=0.5):\n",
        "    sentences = sent_tokenize(text)\n",
        "    filtered_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_embedding = create_embeddings(sentence)\n",
        "        similarities = [F.cosine_similarity(sentence_embedding.unsqueeze(0), keyword_emb.unsqueeze(0), dim=1) for keyword_emb in keyword_embeddings]\n",
        "        if not any(similarity > threshold for similarity in similarities):\n",
        "            filtered_sentences.append(sentence)\n",
        "\n",
        "    processed_text = ' '.join(filtered_sentences)\n",
        "    return preprocess_text(processed_text, stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmUhe16m7a8s",
        "outputId": "d7c70435-6d51-4677-9068-217944eaebeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Keyword Extraction**"
      ],
      "metadata": {
        "id": "aaFml7UhGKcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "g12tw5Voe5sp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd1fc71-f0f6-496c-8593-8532788443fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keybert in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from keybert) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.23.5)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.16.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.16.0+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.19.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (23.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.1.7)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.19.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "import torch\n",
        "\n",
        "# Initialize KeyBERT model\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "def extract_keywords(text, num_keywords=5):\n",
        "    \"\"\"\n",
        "    Extract keywords from a text using KeyBERT and compute their embeddings.\n",
        "\n",
        "    :param text: The text to extract keywords from.\n",
        "    :param num_keywords: Number of keywords to extract.\n",
        "    :return: A dictionary with keyword embeddings and the list of keywords.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract keywords from the text\n",
        "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words='english', top_n=num_keywords, use_mmr=True, diversity=0.7)\n",
        "\n",
        "    # Extract just the keywords (first element of each tuple)\n",
        "    extracted_keywords = [keyword[0] for keyword in keywords]\n",
        "\n",
        "    # Tokenize and encode extracted keywords in a batch\n",
        "    key_word_tokens = tokenizer(extracted_keywords, padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        key_word_outputs = model(**key_word_tokens)\n",
        "    key_word_embeddings = key_word_outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "    return {\n",
        "        'key_word_embeddings': key_word_embeddings,\n",
        "        'keywords': extracted_keywords\n",
        "    }"
      ],
      "metadata": {
        "id": "oDTbOnOG2a1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vUGCuRUgnEE"
      },
      "source": [
        "## **Text Metrics**\n",
        "**Total Words:** The total count of words in the text.\n",
        "\n",
        "**Overall Lexical Diversity:** The ratio of unique words to the total number of words, providing a measure of the text's vocabulary variety.\n",
        "\n",
        "**Average Sentence Lexical Diversity:** The average diversity of vocabulary used across all sentences in the text.\n",
        "\n",
        "**Top Ten Most Frequent Word:** A list of the ten most commonly used words in the text, along with their frequencies.\n",
        "\n",
        "**Total Number of Sentences:** The total sentence count of the text. When analysing processed text, this metric becomes redundant as there is no punctuation to split the text on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STuEJkn-4aXC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def text_metrics(text):\n",
        "    \"\"\"\n",
        "    Calculate various metrics from the given text.\n",
        "\n",
        "    :param text: The input text to analyze\n",
        "    :return: A dictionary with metrics including total words, overall lexical diversity,\n",
        "             average sentence lexical diversity, top ten words, and number of sentences\n",
        "    \"\"\"\n",
        "\n",
        "    # Overall lexical diversity\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    lexical_diversity = overall_lexical_diversity(words)\n",
        "    num_words = len(words)\n",
        "\n",
        "    # Sentence tokenization\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Lexical diversity per sentence\n",
        "    sentence_diversities = []\n",
        "    for sentence in sentences:\n",
        "        words_in_sentence = word_tokenize(sentence)\n",
        "        unique_words = len(set(words_in_sentence))\n",
        "        total_words = len(words_in_sentence)\n",
        "\n",
        "        if total_words > 0:\n",
        "            sentence_diversity = unique_words / total_words\n",
        "        else:\n",
        "            sentence_diversity = 0  # or continue to skip the sentence\n",
        "\n",
        "        sentence_diversities.append(sentence_diversity)\n",
        "\n",
        "    # Average lexical diversity of sentences\n",
        "    avg_sentence_lexical_diversity = sum(sentence_diversities) / len(sentence_diversities) if sentence_diversities else 0\n",
        "\n",
        "    # Top ten most frequent words\n",
        "    top_ten_words = Counter(words).most_common(10)\n",
        "\n",
        "    # Number of sentences\n",
        "    num_sentences = len(sentences)\n",
        "\n",
        "    return {\n",
        "        'num_words': num_words,\n",
        "        'lexical_diversity': lexical_diversity,\n",
        "        'avg_sentence_lexical_diversity': avg_sentence_lexical_diversity,\n",
        "        'top_ten_words': top_ten_words,\n",
        "        'num_sentences': num_sentences\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def overall_lexical_diversity(words):\n",
        "    \"\"\"\n",
        "    Calculate the overall lexical diversity of the text.\n",
        "\n",
        "    :param words: List of all words in the text.\n",
        "    :return: Lexical diversity, a ratio of unique words to total words.\n",
        "    \"\"\"\n",
        "    num_words = len(words)\n",
        "    unique_words = len(set(words))\n",
        "    return unique_words / num_words if num_words > 0 else 0"
      ],
      "metadata": {
        "id": "-Vr-MnCl7f1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKvarCUatq2E"
      },
      "source": [
        "###**Formatting for Text Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bFtign7tLu9"
      },
      "outputs": [],
      "source": [
        "def format_metrics(title, metrics):\n",
        "    # Handling edge case where there might not be any frequent words\n",
        "    if metrics['top_ten_words']:\n",
        "        formatted_top_words = ', '.join([word for word, _ in metrics['top_ten_words']])\n",
        "        highest_word, highest_freq = metrics['top_ten_words'][0]  # Extracting the highest frequency word and its frequency\n",
        "    else:\n",
        "        formatted_top_words = \"None\"\n",
        "        highest_word, highest_freq = (\"N/A\", 0)\n",
        "\n",
        "    # Formatting the diversities as percentages\n",
        "    overall_diversity_percentage = metrics['lexical_diversity'] * 100\n",
        "    avg_sentence_diversity_percentage = metrics['avg_sentence_lexical_diversity'] * 100\n",
        "\n",
        "    return (f\"--------- Text Metrics for {title} ---------\\n\"\n",
        "            f\"Total Words: {metrics['num_words']}\\n\"\n",
        "            f\"Total Sentences: {metrics['num_sentences']}\\n\"\n",
        "            f\"Overall Lexical Diversity: {overall_diversity_percentage:.2f}%\\n\"\n",
        "            f\"Average Lexical Diversity of Sentences: {avg_sentence_diversity_percentage:.2f}%\\n\"\n",
        "            f\"Top Ten Most Frequent Words: {formatted_top_words}\\n\"\n",
        "            f\"Highest Frequency Word: '{highest_word}' (Frequency: {highest_freq})\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dk5c65Ieyu5"
      },
      "source": [
        "## **Importing and Reading `TP001.txt` from URL and `austen-emma.txt` from NLTK corpora**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9jh2IuW8ytE",
        "outputId": "780195df-5060-458e-8675-c888666dd7ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-18 12:07:05--  https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/the-current/tp001.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 220746 (216K) [text/plain]\n",
            "Saving to: ‘tp001.txt.2’\n",
            "\n",
            "\rtp001.txt.2           0%[                    ]       0  --.-KB/s               \rtp001.txt.2         100%[===================>] 215.57K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-11-18 12:07:06 (6.40 MB/s) - ‘tp001.txt.2’ saved [220746/220746]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/the-current/tp001.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hNn54koW82et",
        "outputId": "022f4ba4-7e25-4445-ef9e-79f365abefd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'... we need to work hard to make it happen 3d is better than other bands in the whole country a ban '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 215
        }
      ],
      "source": [
        "# Open the file and read its lines\n",
        "with open('tp001.txt', 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "tp001_text = \"\"\n",
        "for line in lines:\n",
        "    if '\\t' in line:\n",
        "        comment = line.split('\\t')[1].strip()  # Extract and strip the comment\n",
        "        tp001_text += comment + \" \"  # Add the comment to the text string\n",
        "\n",
        "tp001_text[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEbU2LnzDSPT",
        "outputId": "0e4da9cf-0cd3-48c7-a365-a6c60cbdfd89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Downloading gutenberg corpus\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "# Using Emma by Jane Austen 1816\n",
        "emma_text = gutenberg.raw('austen-emma.txt')\n",
        "print(emma_text[:290])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xjsTV0Ri3hI"
      },
      "source": [
        "# **Experimentation**\n",
        "The following experimentation section includes:\n",
        "- An analysis and overview of metrics from both sample texts.  \n",
        "- Visualization of the top ten words before and after processing.\n",
        "- Analysis of Emma's overall lexical diversity before and after processing.\n",
        "\n",
        "**Notes:** I have chosen to use the NLTK's stopword list for preprocessing. I  have used 'Emma by Jane Austen 1816' from NLTK corpora and 'TP001 (Petrol cars should be banned by 2030)' from The Current."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbO_HHU0R2iV"
      },
      "source": [
        "### **Importing libraries and initializing stopwords set**\n",
        "Required for preprocessing and visualization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords from NLTK\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Additional words to the stopwords set\n",
        "additional_stopwords = {'n', 'mr', '1816', 'mrs', 'miss'}\n",
        "stop_words.update(additional_stopwords)\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUBrZP_GgD-0",
        "outputId": "b35093ea-8e02-4544-b60d-527950bc8cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'an', 'did', 'is', 'off', 'my', 'yourselves', 'out', 'hadn', 'can', 'now', 'hers', 'when', 'further', 'i', 'this', 'our', 'has', 'until', 'if', 'won', \"mightn't\", 'of', 'up', 'isn', 'mustn', 'nor', 'what', 'was', 'his', \"you'd\", 'm', 'herself', 'shouldn', 'only', 'between', 'yourself', 'that', 'so', 'have', 'which', \"she's\", 'were', 'being', 'all', 'these', \"weren't\", \"isn't\", 'more', 'at', 'from', 'll', \"wasn't\", 'wouldn', 'its', \"shouldn't\", 'she', 'ours', 'be', 'didn', 'he', \"don't\", 'where', \"you'll\", 'been', \"hasn't\", 'are', 'theirs', 'a', \"aren't\", 'mrs', 'needn', 'because', 'same', 'those', 'below', 'on', \"needn't\", 'themselves', 'again', 'some', \"couldn't\", 'you', 'does', 'into', 'just', 'will', 'against', 'too', 'during', 'ain', 'than', 'while', \"hadn't\", 'whom', 'it', 'their', 'under', \"it's\", 'by', 'then', 'there', 'doesn', 'd', 'don', 'me', 'they', 'doing', 'ourselves', 'for', 'having', 'with', 'each', 'or', 'her', 'over', \"you're\", \"you've\", 'as', 'yours', 'itself', 'how', 'mightn', 'but', 'couldn', \"doesn't\", 'through', 'to', 'them', \"wouldn't\", 'any', 'about', 've', 're', 's', 'above', 'y', 'not', 'wasn', 'myself', 'here', 'we', 'who', \"won't\", 'haven', 'do', 'in', 'why', 'no', 'once', 't', 'other', 'ma', 'himself', \"that'll\", 'the', '1816', 'weren', 'before', 'after', 'down', 'aren', 'very', \"haven't\", 'o', 'own', \"didn't\", 'am', 'your', 'him', 'n', 'most', 'should', 'and', 'few', 'mr', 'had', 'shan', \"should've\", \"shan't\", \"mustn't\", 'both', 'miss', 'hasn', 'such'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBI2_fzUQlXf"
      },
      "source": [
        "## **Analysis and overview of metrics from both sample texts**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98VOzO9MqP1k",
        "outputId": "b56de521-1857-4691-a5e4-dc4fc0bb7e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- Text Metrics for Emma - Raw Text ---------\n",
            "Total Words: 161983\n",
            "Total Sentences: 7493\n",
            "Overall Lexical Diversity: 4.48%\n",
            "Average Lexical Diversity of Sentences: 90.89%\n",
            "Top Ten Most Frequent Words: to, the, and, of, i, a, it, her, was, she\n",
            "Highest Frequency Word: 'to' (Frequency: 5239)\n",
            "\n",
            "--------- Text Metrics for TP001 - Raw Text ---------\n",
            "Total Words: 39065\n",
            "Total Sentences: 1431\n",
            "Overall Lexical Diversity: 12.06%\n",
            "Average Lexical Diversity of Sentences: 88.95%\n",
            "Top Ten Most Frequent Words: the, to, we, and, i, it, a, is, be, for\n",
            "Highest Frequency Word: 'the' (Frequency: 1507)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Analyzing raw texts\n",
        "# --------------------\n",
        "\n",
        "# Get text metrics for raw unprocessed text\n",
        "emma_metrics = text_metrics(emma_text)\n",
        "tp001_metrics = text_metrics(tp001_text)\n",
        "\n",
        "# Extracting top ten words and their frequencies for plotting\n",
        "emma_top_ten_words, emma_frequencies = zip(*emma_metrics['top_ten_words'])\n",
        "tp001_top_ten_words, tp001_frequencies = zip(*tp001_metrics['top_ten_words'])\n",
        "\n",
        "# Extract the number of sentences\n",
        "emma_num_sentences = emma_metrics['num_sentences']\n",
        "tp001_num_sentences = tp001_metrics['num_sentences']\n",
        "\n",
        "# Printing metrics for raw texts\n",
        "print(format_metrics(\"Emma - Raw Text\", emma_metrics))\n",
        "print(format_metrics(\"TP001 - Raw Text\", tp001_metrics))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting keywords for preprocessing\n",
        "print(\"\\033[93mExtracting Keywords...\\033[0m\")\n",
        "\n",
        "emma_keywords = extract_keywords(emma_text, 20)\n",
        "print(emma_keywords['keywords'])\n",
        "\n",
        "tp001_keywords = extract_keywords(tp001_text, 20)\n",
        "print(tp001_keywords['keywords'])\n",
        "\n",
        "emma_keyword_embeddings = emma_keywords['key_word_embeddings']\n",
        "tp001_keyword_embeddings = tp001_keywords['key_word_embeddings']\n",
        "\n",
        "print(\"\\033[92mExtraction Successful\\n\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huw-7WZD3ZJR",
        "outputId": "04abb8ee-44d4-49d3-cb8b-a05d79b06295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mExtracting Keywords...\u001b[0m\n",
            "['emma', 'inheritance', 'extenuations', 'beaufet', 'unison', 'disinterestedness', 'refined', 'bounded', 'sooner', 'slumbering', 'residence', 'confirmed', '_try_', 'fever', 'gallantry', 'thankfully', 'unobjectionable', 'doctrines', 'plotting', 'invitations']\n",
            "['renewables', 'alternatives', 'tecnology', 'earthfirfuturegeb', '2023', 'limit', 'inalterbative', 'agree', 'scoot', 'subsidising', 'react', 'extraction', 'diseases', 'carscontribute', 'enable', 'havent', 'pedestrians', 'sure', 'ahhhhhhhhhhhh', 'average']\n",
            "\u001b[92mExtraction Successful\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing preprocessed texts\n",
        "# ----------------------------\n",
        "\n",
        "# def refine_text(text, keywords, stop_words, threshold=0.5):\n",
        "\n",
        "preprocessed_tp001 = refine_text(tp001_text, tp001_keyword_embeddings, stop_words)\n",
        "preprocessed_tp001_metrics = text_metrics(preprocessed_tp001)\n",
        "print(format_metrics(\"TP001 - Preprocessed Text\", preprocessed_tp001_metrics))\n",
        "\n",
        "preprocessed_emma = refine_text(emma_text, emma_keyword_embeddings, stop_words)\n",
        "preprocessed_emma_metrics = text_metrics(preprocessed_emma)\n",
        "print(format_metrics(\"Emma - Preprocessed Text\", preprocessed_emma_metrics))\n",
        "\n",
        "# Extracting top ten words and their frequencies for preprocessed texts\n",
        "preprocessed_emma_top_ten, preprocessed_emma_freq = zip(*preprocessed_emma_metrics['top_ten_words'])\n",
        "preprocessed_tp001_top_ten, preprocessed_tp001_freq = zip(*preprocessed_tp001_metrics['top_ten_words'])\n",
        "\n",
        "# Extract the number of sentences for preprocessed texts\n",
        "preprocessed_emma_num_sentences = preprocessed_emma_metrics['num_sentences']\n",
        "preprocessed_tp001_num_sentences = preprocessed_tp001_metrics['num_sentences']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ofly4OM3W_M",
        "outputId": "f283d269-38f4-47f1-de2b-439e63ebc183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- Text Metrics for TP001 - Preprocessed Text ---------\n",
            "Total Words: 1255\n",
            "Total Sentences: 1\n",
            "Overall Lexical Diversity: 52.19%\n",
            "Average Lexical Diversity of Sentences: 52.47%\n",
            "Top Ten Most Frequent Words: cars, need, change, petrol, think, would, future, time, electric, good\n",
            "Highest Frequency Word: 'cars' (Frequency: 22)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfqQ-qJwRFnI"
      },
      "source": [
        "## **Visualization of The Top Ten Words with Their Frequencies Before and After Processing**\n",
        "\n",
        "The visual comparison of word frequencies before and after text processing illustrates the shift from generic to specific language elements, informing the thematic interpretation of the text.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ],
      "metadata": {
        "id": "OFHAklIN9NZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gMxQcxkFv03"
      },
      "outputs": [],
      "source": [
        "# Create a subplot figure with 2 rows and 2 columns\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        'Emma Before Processing',\n",
        "        '\"Petrol cars should be banned by 2030\" Before Processing',\n",
        "        'Emma After Processing',\n",
        "        '\"Petrol cars should be banned by 2030\" After Processing')\n",
        ")\n",
        "\n",
        "# Original Emma\n",
        "fig.add_trace(\n",
        "    go.Bar(x=emma_top_ten_words, y=emma_frequencies),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Original TP001\n",
        "fig.add_trace(\n",
        "    go.Bar(x=tp001_top_ten_words, y=tp001_frequencies),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Preprocessed Emma\n",
        "fig.add_trace(\n",
        "    go.Bar(x=preprocessed_emma_top_ten, y=preprocessed_emma_freq),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Preprocessed TP001\n",
        "fig.add_trace(\n",
        "    go.Bar(x=preprocessed_tp001_top_ten, y=preprocessed_tp001_freq),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title_text='Top Ten Words and Their Frequencies',\n",
        "    showlegend=False,\n",
        "    height=800, width=1200\n",
        ")\n",
        "\n",
        "# Customize axis labels\n",
        "fig.update_xaxes(title_text='Words', row=1, col=1)\n",
        "fig.update_xaxes(title_text='Words', row=1, col=2)\n",
        "fig.update_xaxes(title_text='Words', row=2, col=1)\n",
        "fig.update_xaxes(title_text='Words', row=2, col=2)\n",
        "fig.update_yaxes(title_text='Occurrence Frequency', col=1)\n",
        "fig.update_yaxes(title_text='Occurrence Frequency', col=2)\n",
        "\n",
        "# Show the figure\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CnO9l6vbV9b"
      },
      "source": [
        "## **Comparative Analysis of Overall Lexical Diversity in Processed and Unprocessed Versions of \"Emma\" by Jane Austen.**\n",
        "\n",
        "The results of the analysis below show the overall lexical diversity of Jane Austen's \"Emma\" in both its processed and unprocessed forms as the batch size (number of sentences per batch) increases. It demonstrates how lexical diversity decreases as the size of the text increases.\n",
        "\n",
        "**Processed Overall Lexical Diversity (Blue):** As the batch size increases, we observe a gradual decrease in lexical diversity for the processed version. This trend indicates that when analyzing larger portions of the text together, the processed version becomes less lexically diverse. This may be attributed to the removal of high-frequency words during processing, which results in a more focused vocabulary.\n",
        "\n",
        "**Unprocessed Overall Lexical Diversity (Red):** The red line represents the lexical diversity of the original, unprocessed text. Similarly, as the batch size increases, we also see a decrease in lexical diversity for the unprocessed version. This decrease suggests that even in the unprocessed text, certain words become more prominent and repetitive when analyzing larger sections of the text. However, the unprocessed text tends to maintain a higher lexical diversity compared to the processed version, as it retains all words without filtering.\n",
        "\n",
        "In summary, the overall lexical diversity highlights how text processing affects the diversity of words in a text, particularly as the scale increases. While both processed and unprocessed texts exhibit a decrease in lexical diversity with larger batch sizes, the processed version tends to show a more pronounced decrease due to the removal of high-frequency words. Understanding these trends can aid in choosing the appropriate preprocessing strategy based on the specific goals of text analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrH5B2OcbdOT"
      },
      "outputs": [],
      "source": [
        "increment = 50  # n sentences per increment\n",
        "batch_sizes = list(range(1, emma_num_sentences, increment))  # Incrementally increase batch size\n",
        "\n",
        "overall_ld_unprocessed = []\n",
        "overall_ld_processed = []\n",
        "\n",
        "# Split the text into sentences\n",
        "sentences = sent_tokenize(emma_text)\n",
        "\n",
        "# Calculate lexical diversities\n",
        "for batch_size in batch_sizes:\n",
        "    unprocessed = ' '.join(sentences[:batch_size])\n",
        "    processed = preprocess_text(unprocessed, stop_words)\n",
        "\n",
        "    # Extract words and calculate lexical diversity\n",
        "    unprocessed_words = re.findall(r'\\b\\w+\\b', unprocessed.lower())\n",
        "    processed_words = re.findall(r'\\b\\w+\\b', processed.lower())\n",
        "\n",
        "    unprocessed_diversity = overall_lexical_diversity(unprocessed_words)\n",
        "    processed_diversity = overall_lexical_diversity(processed_words)\n",
        "\n",
        "    overall_ld_unprocessed.append(unprocessed_diversity)\n",
        "    overall_ld_processed.append(processed_diversity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kncVNwO_y_L3"
      },
      "outputs": [],
      "source": [
        "# Prepare batch size labels with sentence count\n",
        "batch_size_labels = [batch_size for batch_size in batch_sizes]\n",
        "\n",
        "# Convert lexical diversity to percentages\n",
        "processed_lex_div = [ld * 100 for ld in overall_ld_processed]\n",
        "unprocessed_lex_div = [ld * 100 for ld in overall_ld_unprocessed]\n",
        "\n",
        "# Create traces\n",
        "trace1 = go.Scatter(\n",
        "    x=batch_size_labels,\n",
        "    y=processed_lex_div,\n",
        "    mode='lines',\n",
        "    name='Overall Lexical Diversity (Processed)',\n",
        ")\n",
        "trace2 = go.Scatter(\n",
        "    x=batch_size_labels,\n",
        "    y=unprocessed_lex_div,\n",
        "    mode='lines',\n",
        "    name='Overall Lexical Diversity (Unprocessed)',\n",
        ")\n",
        "\n",
        "# Layout\n",
        "layout = go.Layout(\n",
        "    title='Overall Lexical Diversity over Increments of ' + str(increment) + ' Sentences',\n",
        "    xaxis=dict(title='Number of Sentences'),\n",
        "    yaxis=dict(title='Lexical Diversity (%)'),\n",
        ")\n",
        "\n",
        "# Figure\n",
        "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
        "\n",
        "# Show plot\n",
        "fig.show()\n",
        "\n",
        "# Print metrics (assuming format_metrics is defined)\n",
        "print(\"\\n\" + format_metrics(\"Emma (Unprocessed)\", emma_metrics) + \"\\n\")\n",
        "print(format_metrics(\"Emma (Processed)\", preprocessed_emma_metrics) + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOU6guqf8qAttk8LaqbdhKL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}