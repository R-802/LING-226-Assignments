{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfiuYkxfA78tznm7eBWddQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R-802/LING-226-Assignments/blob/main/LING226_2023T3_Assignment_One.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Shemaiah Rangitaawa\n",
        "- 300601546\n",
        "- Attempting Challenge"
      ],
      "metadata": {
        "id": "tDF9NZcFWpFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing Function `preprocess_text`\n",
        "\n",
        "This function is designed to preprocess text data for natural language processing tasks. The preprocessing steps include:\n",
        "\n",
        "1. **Remove Punctuation**: The function strips all punctuation from the text. Punctuation often doesn't contribute to the meaning of text for many NLP tasks.\n",
        "\n",
        "2. **Remove Stopwords**: Stopwords (common words that typically don't contribute much meaning, like \"the\", \"is\", \"at\") are removed from the text. This helps in focusing on words that carry more significance.\n",
        "\n",
        "3. **Lowercase All Words**: The text is converted to lowercase. This standardization is crucial as it prevents the same words in different cases from being counted as different words (e.g., \"Hello\" and \"hello\").\n",
        "\n",
        "4. **Remove Words Below/Above a Certain Frequency**: Words that appear very rarely or very frequently in the dataset can be removed. This threshold can be set as per the requirements of the task. Rare words might be typos or irrelevant, and very common words might not carry useful information.\n"
      ],
      "metadata": {
        "id": "PXj8RZ3HXJnX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyQLnGe6WdNB"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text, stopwords, random_frequency):\n",
        "    \"\"\"\n",
        "    Preprocesses the given text by removing punctuation, stopwords, making it lowercase,\n",
        "    and removing words that occur more than the specified random frequency.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The input text to be preprocessed.\n",
        "    stopwords (set): A set of stopwords to be removed.\n",
        "    random_frequency (int): Threshold frequency for word removal.\n",
        "\n",
        "    Returns:\n",
        "    str: The preprocessed text.\n",
        "    list: The words removed based on exceeding the random frequency.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove punctuation and make lowercase\n",
        "    text = ''.join([char.lower() for char in text if char.isalnum() or char.isspace()])\n",
        "    words = text.split()\n",
        "\n",
        "    # Count the frequency of each word\n",
        "    word_frequency = {}\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            word_frequency[word] = word_frequency.get(word, 0) + 1\n",
        "\n",
        "    # Remove words that occur more than random_frequency times\n",
        "    processed_words = []\n",
        "    removed_words = []\n",
        "    for word in words:\n",
        "        if word_frequency.get(word, 0) > random_frequency:\n",
        "            if word not in removed_words:\n",
        "                removed_words.append(word)\n",
        "        else:\n",
        "            processed_words.append(word)\n",
        "\n",
        "    return ' '.join(processed_words), removed_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `print_removed`\n",
        "   1. Begins by breaking down the preprocessed text into individual words.\n",
        "   2. It examines and accumulates the characters that were removed between these words.\n",
        "   3. Then it returns these removed characters as a single string"
      ],
      "metadata": {
        "id": "Fw7y0Ot2eq1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def print_removed(original_text, preprocessed_text):\n",
        "    \"\"\"\n",
        "    Analyzes the differences between the original and preprocessed texts to attempt to infer the word removal frequency.\n",
        "\n",
        "    Parameters:\n",
        "    original_text (str): The original text before preprocessing.\n",
        "    preprocessed_text (str): The text after it has been preprocessed.\n",
        "\n",
        "    Returns:\n",
        "    removed_characters (str): The characters removed during preprocessing.\n",
        "    removed_words (list): List of removed words.\n",
        "    inferred_frequency (int): Inferred word removal frequency.\n",
        "    \"\"\"\n",
        "    # Process original text\n",
        "    original_text_processed = ''.join([char.lower() for char in original_text if char.isalnum() or char.isspace()])\n",
        "    original_words = original_text_processed.split()\n",
        "\n",
        "    # Split the preprocessed text into words\n",
        "    preprocessed_words = preprocessed_text.split()\n",
        "\n",
        "    # Count frequency of words in the original text\n",
        "    original_word_freq = Counter(original_words)\n",
        "\n",
        "    # Calculate frequency of removed words\n",
        "    removed_words_freq = original_word_freq - Counter(preprocessed_words)\n",
        "    removed_words = list(removed_words_freq.keys())\n",
        "\n",
        "    # Attempt to infer the frequency\n",
        "    if removed_words_freq:\n",
        "        # Calculate the average frequency of removed words\n",
        "        avg_freq = sum(removed_words_freq.values()) / len(removed_words_freq)\n",
        "        inferred_frequency = round(avg_freq)\n",
        "    else:\n",
        "        inferred_frequency = None\n",
        "\n",
        "    # Calculate removed characters\n",
        "    removed_characters = ''.join(set(original_text) - set(preprocessed_text))\n",
        "\n",
        "    return removed_characters, removed_words, inferred_frequency"
      ],
      "metadata": {
        "id": "V3NuXgCpeqI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing NLTK Stopwords as Set And Usage Example"
      ],
      "metadata": {
        "id": "0raHbqCCZnYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Convert NLTK's stopwords to a set\n",
        "stopwords_set = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khPj6yjoZZ5z",
        "outputId": "00b13d23-f8e3-447e-9543-a34bf29c6e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"This is an example text demonstrating text preprocessing's importance.\n",
        "It includes various words, including common stopwords. Preprocessing techniques\n",
        "like tokenization, removing stopwords, and stemming are essential for converting\n",
        "raw text into analyzable format in NLP. In NLP, we encounter diverse data, from\n",
        "social media to research papers, each with unique challenges. Text preprocessing\n",
        "helps clean and prepare data for extracting insights. Preprocessing methods may\n",
        "vary based on the NLP task. Understanding and applying these techniques are\n",
        "fundamental for extracting valuable information from text.\"\"\""
      ],
      "metadata": {
        "id": "WwpNPWvyXlEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random_frequency = random.randint(1, 10) # Generate random word removal frequency\n",
        "preprocessed_text, removed_words_list = preprocess_text(text, stopwords_set, random_frequency)\n",
        "\n",
        "print(\"Raw Text:\")\n",
        "print(text)\n",
        "\n",
        "print(\"\\nProcessed Text:\")\n",
        "print(preprocessed_text)\n",
        "\n",
        "# Call the function and store the results\n",
        "removed_characters, removed_words, inferred_frequency = print_removed(text, preprocessed_text)\n",
        "\n",
        "# Print the removed characters\n",
        "print(\"\\nCharacters Removed During Preprocessing:\")\n",
        "print(removed_characters)\n",
        "\n",
        "# Print the inferred frequency\n",
        "print(f\"\\nInferred Removal Frequency: {inferred_frequency}\")\n",
        "\n",
        "# Print the actual removal frequency\n",
        "print(f\"Actual Removal Frequency: {random_frequency}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl933uXamW2j",
        "outputId": "5456ed77-181f-4ccd-c24a-8d24aff0a723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Text:\n",
            "This is an example text demonstrating text preprocessing's importance. \n",
            "It includes various words, including common stopwords. Preprocessing techniques \n",
            "like tokenization, removing stopwords, and stemming are essential for converting \n",
            "raw text into analyzable format in NLP. In NLP, we encounter diverse data, from \n",
            "social media to research papers, each with unique challenges. Text preprocessing \n",
            "helps clean and prepare data for extracting insights. Preprocessing methods may\n",
            "vary based on the NLP task. Understanding and applying these techniques are\n",
            "fundamental for extracting valuable information from text.\n",
            "\n",
            "Processed Text:\n",
            "this is an example text demonstrating text preprocessings importance it includes various words including common stopwords preprocessing techniques like tokenization removing stopwords and stemming are essential for converting raw text into analyzable format in nlp in nlp we encounter diverse data from social media to research papers each with unique challenges text preprocessing helps clean and prepare data for extracting insights preprocessing methods may vary based on the nlp task understanding and applying these techniques are fundamental for extracting valuable information from text\n",
            "\n",
            "Characters Removed During Preprocessing:\n",
            ".PI\n",
            "UN,'TL\n",
            "\n",
            "Inferred Removal Frequency: None\n",
            "Actual Removal Frequency: 5\n"
          ]
        }
      ]
    }
  ]
}